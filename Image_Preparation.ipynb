{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image preparation\n",
    "\n",
    "The original image size is 55x90 pixels with a color depth of 3 (RGB).\n",
    "The below code can be used to transform the images in an input directory (Input_dir) to the right size (20x32 pixels) into an output directory (Output_dir). Inside the directory the pictures are stored in subdirectories according their labeling (0 ... 9 + NaN).\n",
    "Any other image converter can be used as well.\n",
    "\n",
    "### Prerequisite\n",
    "Installed OpenCV libary within python (opencv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from PIL import Image \n",
    "\n",
    "Input_dir = 'data_raw_all'\n",
    "Output_dir= 'data_resize_all'\n",
    "\n",
    "target_size_x = 32\n",
    "target_size_y = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7684 files have been deleted.\n"
     ]
    }
   ],
   "source": [
    "files = glob.glob(Output_dir + '/*.jpg')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "print(str(len(files)) + \" files have been deleted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data_raw_all\\0.0_0.0.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muell\\AppData\\Local\\Temp\\ipykernel_26424\\2001317934.py:14: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  test_image = test_image.resize((target_size_x, target_size_y), Image.NEAREST)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 data_raw_all\\0.7_0273_zeiger1_2020-04-29_12-21-02.jpg\n",
      "1000 data_raw_all\\1.3_0565_zeiger4_2020-04-29_11-44-02.jpg\n",
      "1500 data_raw_all\\1.9_0886_zeiger3_2019-09-14_21-00-12.jpg\n",
      "2000 data_raw_all\\2.5_2f0b6991fad6e5308dd6bf5fffa0bede.jpg\n",
      "2500 data_raw_all\\3.2_1532_zeiger1_2020-04-29_14-02-02.jpg\n",
      "3000 data_raw_all\\3.8_93d5002d7c0c697d8437dc6db7d9b995.jpg\n",
      "3500 data_raw_all\\4.4_pointer_20211008-080205.jpg\n",
      "4000 data_raw_all\\5.1_021a729de1f0b2df4a2f3dc792e0e806.jpg\n",
      "4500 data_raw_all\\5.7_e50495abc5bf227f64168bf61451db77.jpg\n",
      "5000 data_raw_all\\6.4_3110_zeiger1_2019-09-14_21-20-13.jpg\n",
      "5500 data_raw_all\\7.0_3597_zeiger3_2019-11-19_16-52-03.jpg\n",
      "6000 data_raw_all\\7.8_026949f0099cb7a3970423ec929b3916.jpg\n",
      "6500 data_raw_all\\8.5_19ff6680c7934b07ba3230441521a6d3.jpg\n",
      "7000 data_raw_all\\9.1_4532_zeiger2_2019-06-02T183009.jpg\n",
      "7500 data_raw_all\\9.7_4892_zeigeru__2019-06-05T09000.jpg\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "files = glob.glob(Input_dir + '/*.jpg')\n",
    "hashes={}\n",
    "for i,aktfile in enumerate(files):\n",
    "    if i%500==0:\n",
    "        print(i, aktfile)\n",
    "    test_image = Image.open(aktfile)\n",
    "    hash=hashlib.sha256(test_image.tobytes()).hexdigest()\n",
    "    if hash in hashes:\n",
    "        hashes[hash].append(aktfile)\n",
    "    else:\n",
    "        hashes[hash]=[aktfile]\n",
    "    test_image = test_image.resize((target_size_x, target_size_y), Image.NEAREST)\n",
    "    base=os.path.basename(aktfile)\n",
    "    save_name = Output_dir + '/' + base\n",
    "    test_image.save(save_name, \"JPEG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing duplicate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# duplicate files are a risk to the metrics, they pollute the validation dataset\n",
    "for hash in hashes:\n",
    "    if len(hashes[hash])>1:\n",
    "        print(hashes[hash])    \n",
    "        for duplicate in hashes[hash][1:]:\n",
    "            # remove all except the first\n",
    "            os.remove(duplicate)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "vscode": {
   "interpreter": {
    "hash": "3431513051aa6ef6a02e4978d18932220794b1f41fa21fbae2ad068757488314"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
