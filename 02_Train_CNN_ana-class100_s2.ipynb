{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training\n",
    "\n",
    "Target of this code is to train a CNN network to extract the needle position of an analog needle device.\n",
    "\n",
    "### Preparing the training\n",
    "* First all libraries are loaded\n",
    "    * It is assumed, that they are installed during the Python setup\n",
    "* matplotlib is set to print the output inline in the jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Model naming\n",
    "TFlite_MainType: str = 'ana-class100'\n",
    "TFlite_Version: str  = 'undefined'\n",
    "TFlite_Size: str     = 's1'\n",
    "\n",
    "# Validation size\n",
    "# Note: 0.0 = 0% validation size, use all images for training\n",
    "Validation_Percentage = 0.2\n",
    "\n",
    "# Folders\n",
    "Input_Dir: str  = 'data_resize_all'\n",
    "Output_Dir: str = 'models/ana-class100'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import pi\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from src.utils.plot_functions import plot_dataset, plot_dataset_analog\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "# Make sure version is 4 characters long if version is defined as int (e.g. papermill paramter 100 -> 0100)\n",
    "if isinstance(TFlite_Version, int):\n",
    "    TFlite_Version = str(TFlite_Version).zfill(4)\n",
    "\n",
    "\n",
    "# Prepare folders\n",
    "if not (Path(Input_Dir).exists() and Path(Input_Dir).is_dir()): # Check if input is availabe\n",
    "    sys.exit(f\"Aborting: Folder '{Input_Dir}' does not exist.\")\n",
    "    \n",
    "Path(Output_Dir).mkdir(parents=True, exist_ok=True)  # Create output folder if it doesn't exist\n",
    "\n",
    "\n",
    "# Disable GPUs\n",
    "try:\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    visible_devices = tf.config.get_visible_devices()\n",
    "    for device in visible_devices:\n",
    "        assert device.device_type != 'GPU'\n",
    "except:\n",
    "    # Invalid device or cannot modify virtual devices once initialized.\n",
    "    pass\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data\n",
    "* The data is expected in the \"Input_dir\"\n",
    "* Picture size must be 32x32 with 3 color channels (RGB)\n",
    "* The filename contains the informations needed for training in the first 3 digits::\n",
    "* Typical filename: \n",
    "    * x.y-zzzz.jpg \n",
    "    * e.g. \"4.6_Lfd-1406_zeiger3_2019-06-02T050011\"\n",
    "\n",
    "|Place holder | Meaning                     | Usage        |\n",
    "|------------- |-----------------------------|--------------|\n",
    "| **x.y**          | readout value               | **to be learned** |\n",
    "| zzzz        | additional information              | not needed   |\n",
    "\n",
    "* The images are stored in the x_data[]\n",
    "* The expected output for each image in the corresponding y_data[]\n",
    "    * The periodic nature is reflected in a **sin/cos coding**, which allows to restore the angle/counter value with an arctan later on.\n",
    "\n",
    "* The last step is a shuffle (from sklearn.utils) as the filenames are on order due to the encoding of the expected analog readout in the filename "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(f\"{Input_Dir}/*.jpg\")\n",
    "num_files = len(files)\n",
    "\n",
    "f_data = np.empty(num_files, dtype=\"<U250\")\n",
    "x_data = np.empty((num_files, 32, 32, 3))\n",
    "y_data = np.empty(num_files)\n",
    "\n",
    "for i, aktfile in enumerate(files):\n",
    "    base = os.path.basename(aktfile)\n",
    "\n",
    "    # get label from filename (1.2_ new or 1_ old),\n",
    "    if (base[1]==\".\"):\n",
    "        target = base[0:3]\n",
    "    else:\n",
    "        target = base[0:1]\n",
    "    category = float(target)\n",
    "    \n",
    "    test_image = Image.open(aktfile)\n",
    "    test_image = np.array(test_image, dtype=\"float32\")\n",
    "    f_data[i] = aktfile\n",
    "    x_data[i] = test_image\n",
    "    y_data[i] = category\n",
    "\n",
    "print(\"Data count: \", len(y_data))  \n",
    "print(x_data.shape)\n",
    "\n",
    "x_data, y_data, f_data = shuffle(x_data, y_data, f_data)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=Validation_Percentage)\n",
    "y_train = keras.utils.to_categorical(y_train*10, 100)\n",
    "y_test = keras.utils.to_categorical(y_test*10, 100)\n",
    "\n",
    "print(np.expand_dims(y_data, axis=1).shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the data\n",
    "\n",
    "Uneven distribution of data can lead to poorer results.\n",
    "\n",
    "In some classes (8.4) we have only 20 images. In other classes over 100 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, inverse = np.unique(y_data, return_inverse=True)\n",
    "ziffer_bincount = np.bincount(inverse)\n",
    "fig = plt.figure(figsize=(40, 10))\n",
    "fig.suptitle(\"Distribution of data\")\n",
    "plt.bar(np.arange (0, 100/10, 0.1), ziffer_bincount, width=0.09, align='center')\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('digit class')\n",
    "plt.tight_layout()\n",
    "_ = plt.xticks(np.arange(0, 100/10, 0.1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "The layout of the network ist a typcial CNN network with alternating 2x **Conv2D**, **BatchNormalization** and **MaxPool2D** layers. Finished after **flattening** with additional **Dense** layer.\n",
    "\n",
    "**Dropout** between CNN layers are 0.2, between fully connected layers 0.4, but only the fully connected output layer.\n",
    "\n",
    "* Shape of the input layer: (32, 32, 3)\n",
    "* Shape of the output layer: (100) - classification 0.0 ... 9.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(32, 32, 3))\n",
    "x = BatchNormalization()(inputs)\n",
    "\n",
    "x = Conv2D(32, (5, 5), padding='same', activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(4, 4))(x)\n",
    "x = Dropout(0.15)(x)\n",
    "\n",
    "x = Conv2D(32, (3, 3), padding='same', activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.15)(x)\n",
    "\n",
    "x = Conv2D(48, (3, 3), padding='same', activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(4, 4))(x)\n",
    "x = Dropout(0.15)(x)\n",
    "\n",
    "x = Conv2D(48, (3, 3), padding='same', activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.15)(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding='same', activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPool2D(pool_size=(2, 2))(x)\n",
    "x = Dropout(0.15)(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), padding='same', activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "# Output layer for 100-class classification\n",
    "output = Dense(100)(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "The input pictures are randomly scattered for brightness and pixel shift variations. These is implemented with a ImageDataGenerator.\n",
    "\n",
    "\n",
    "The training is in one step. \n",
    "\n",
    "The best model will be stored as \\<modelname\\>_best.h5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "\n",
    "Augmentation uses shift, zoom, brightness, shear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch_Size = 8\n",
    "Shift_Range = 1\n",
    "Brightness_Range = 0.2\n",
    "Zoom_Range = 0.05\n",
    "\n",
    "def random_invert_image(x, probability_invert=0.2):\n",
    "    \"\"\"\n",
    "    Invert an image with a given probability\n",
    "    \"\"\"\n",
    "    if random.random() > probability_invert:\n",
    "        return x\n",
    "    return 255 - x  # Invert image\n",
    "\n",
    "def random_white_balance(x, strength_range=(0.9, 1.1)):\n",
    "    \"\"\"\n",
    "    Simulates poor white balance by randomly scaling RGB channels independently.\n",
    "    strength_range controls how strong the color cast distortion is.\n",
    "    \"\"\"\n",
    "    x = x.astype(np.float32)\n",
    "\n",
    "    # Random scaling for each channel (simulates color cast)\n",
    "    r_scale = np.random.uniform(*strength_range)\n",
    "    g_scale = np.random.uniform(*strength_range)\n",
    "    b_scale = np.random.uniform(*strength_range)\n",
    "\n",
    "    x[..., 0] *= r_scale  # Red channel\n",
    "    x[..., 1] *= g_scale  # Green channel\n",
    "    x[..., 2] *= b_scale  # Blue channel\n",
    "\n",
    "    return x\n",
    "\n",
    "def preprocessing(x):\n",
    "    x = random_invert_image(x)\n",
    "    x = random_white_balance(x)\n",
    "    x = np.clip(x, 0.0, 255.0)\n",
    "    return x.astype(np.float32)\n",
    "\n",
    "# Training data\n",
    "print(\"Training data\")\n",
    "datagen = ImageDataGenerator(width_shift_range=[-Shift_Range, Shift_Range], \n",
    "                             height_shift_range=[-Shift_Range, Shift_Range],\n",
    "                             brightness_range=[1 - Brightness_Range, 1 + Brightness_Range],\n",
    "                             zoom_range=[1 - Zoom_Range, 1 + Zoom_Range],\n",
    "                             channel_shift_range=5,\n",
    "                             shear_range=1,\n",
    "                             preprocessing_function=preprocessing\n",
    "                            )\n",
    "\n",
    "train_iterator = datagen.flow(x_train, y_train, batch_size=Batch_Size)\n",
    "plot_dataset_analog(train_iterator)\n",
    "\n",
    "if (Validation_Percentage > 0):\n",
    "    # Validation data\n",
    "    datagen_val = ImageDataGenerator() # No argumentation for validation\n",
    "    validation_iterator = datagen_val.flow(x_test, y_test, batch_size=Batch_Size)\n",
    "    print(\"  \")\n",
    "    print(\"Validation data\")\n",
    "    plot_dataset_analog(validation_iterator, rows=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Epoch_Anz = 600\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss', \n",
    "    factor=0.9, \n",
    "    patience=10, \n",
    "    min_lr=1e-5, \n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "earlystop = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    mode='min', \n",
    "    patience=40, \n",
    "    verbose=0,  \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_iterator, \n",
    "    validation_data=validation_iterator, \n",
    "    batch_size=Batch_Size, \n",
    "    epochs = Epoch_Anz,\n",
    "    callbacks=[earlystop, reduce_lr],\n",
    "    verbose=0\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result\n",
    " \n",
    "* Visualization of the training and validation results\n",
    "* Validation should not much over train validation, because of the augmentation of train data (validation data not)\n",
    "* Beware, the best model will be used. This can be from an earlier epoch as the last. (mostly the last -30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(history.history['loss'])\n",
    "\n",
    "if (Validation_Percentage > 0):\n",
    "    plt.semilogy(history.history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training','validation'], loc='upper left')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model verification\n",
    "\n",
    "* The following code uses the trained model to check the deviation for each picture (train + validation).\n",
    "* The max_delta can be used to get the accuracy with allowed differences (for instance +/- 0.1)\n",
    "* The first (max) 49 false predicted images will be shown\n",
    "* A csv-file with all false predicted images will be created. It can be used for relabeling. Usage `python3 -m collectmeteranalog  --labelfile=ana-class100_s1_false_predicted.csv --model=ana-class100_s1q.tflite` (see https://github.com/haverland/collectmeteranalog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_delta=0.11\n",
    "\n",
    "def plot_dataset(images, labels, columns=10, rows=5, figsize=(18, 10)):\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    \n",
    "    for i in range(1, columns*rows +1):\n",
    "        if (i>len(labels)):\n",
    "            break\n",
    "        fig.add_subplot(rows, columns, i)\n",
    "        plt.title(labels[i-1])  # set title\n",
    "        plt.imshow((images[i-1]).astype(np.uint8), aspect='1', extent=[0, 1, 0, 1])\n",
    "        # yellow lines\n",
    "        ax=plt.gca()\n",
    "        ax.get_yaxis().set_visible(False) \n",
    "        ax.get_xaxis().set_visible(False) \n",
    "        \n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_divergence(divergationset, title1):\n",
    "    fig = plt.figure(figsize=(40, 10))\n",
    "    fig.suptitle(title1)\n",
    "    plt.bar(np.arange (0, len(divergationset)/10, 0.1), divergationset, width=0.09, align='center')\n",
    "    plt.ylabel('Count')\n",
    "    plt.xlabel('Class')\n",
    "    plt.xticks(np.arange(0, len(divergationset)/10, 0.1))\n",
    "    return fig\n",
    "\n",
    "classes = model.predict(x_data.astype(np.float32))\n",
    "predictions = np.argmax(classes, axis=1).reshape(-1)/10\n",
    "\n",
    "# 9.9 <> 0 = 0.1 and 1.1 <> 1.2 = 0.1\n",
    "differences = np.minimum(np.abs(predictions - y_data), np.abs(predictions - (10 - y_data)))\n",
    "# used for filtering\n",
    "false_differences = differences > max_delta\n",
    "# only differences bigger than delta. so small differences can be ignored in early stages\n",
    "false_predicted = differences[false_differences]\n",
    "false_images = x_data[false_differences]\n",
    "false_labels = [ \"Expected: \" + str(y1) + \"\\n Predicted: \" + str(p) + \"\\n\" + str(f)[-26:-4] for y1, p, \n",
    "                f in zip(y_data[false_differences], predictions[false_differences], f_data[false_differences])]\n",
    "false_files = [ f for f in  f_data[false_differences]]\n",
    "\n",
    "print(f\"Accuracy: {(1 - len(false_predicted) / len(y_data)) * 100.0} % (Images: {len(y_data)} | False Predicted: {len(false_predicted)})\")\n",
    "# plot the differences (max difference can only be 5.0)\n",
    "plot_divergence(np.bincount(np.array(np.round(false_predicted*10)).astype(int), minlength=51), \"False Predicted Divergation\")\n",
    "\n",
    "# plot the false predicted images\n",
    "plot_dataset(np.array(false_images), false_labels, columns=7, rows=7, figsize=(18,18))\n",
    "#print(false_files)\n",
    "fpf = pd.DataFrame(false_files)\n",
    "\n",
    "# the csv can be used with collectmeterdigits to fix labels\n",
    "# python3 -m collectmeteranalog  --labelfile=ana-class10_0168_s1_false_predicted.csv --model=ana-class100_0168_s1_q.tflite\n",
    "fpf.to_csv(f\"{Output_Dir}/{TFlite_MainType}_{TFlite_Version}_{TFlite_Size}\" + '_false_predicted.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "* Save the model to the file with the \"tflite\" file format\n",
    "* quantize the model and store it as -q.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileName = f\"{Output_Dir}/{TFlite_MainType}_{TFlite_Version}_{TFlite_Size}.tflite\"\n",
    "\n",
    "# TensorFlow Lite conversion\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the converted model\n",
    "with open(FileName, \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(f\"Model saved successfully. File: {FileName}\")\n",
    "print(f\"File size: {Path(FileName).stat().st_size} bytes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileName = f\"{Output_Dir}/{TFlite_MainType}_{TFlite_Version}_{TFlite_Size}_q.tflite\"\n",
    "\n",
    "# Representative dataset function\n",
    "def representative_dataset():\n",
    "    for n in range(x_data[0].shape[0]):\n",
    "        data = np.expand_dims(x_data[n], axis=0)\n",
    "        yield [data.astype(np.float32)]\n",
    "\n",
    "# TensorFlow Lite conversion with optimizations\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter._experimental_disable_per_channel_quantization_for_dense_layers = True\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the converted model to the specified file\n",
    "with open(FileName, \"wb\") as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print(f\"Model saved successfully. File: {FileName}\")\n",
    "print(f\"File size: {Path(FileName).stat().st_size} bytes\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tflite2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
